# MetricFoundry

## Supported dataset ingestion

MetricFoundry's staging Lambda and LangGraph analytics pipeline ingest several
widely used structured data formats out of the box. Delimited text files (CSV
and TSV), JSON / JSON Lines payloads, Excel workbooks (`.xls`, `.xlsx`,
`.xlsm`), Parquet files, and SQLite database exports are parsed natively.
Common archive formats including GZIP, ZIP, TAR, and TAR.GZ are expanded during
ingest so nested datasets are normalised without extra user effort. Binary
columns encountered in SQLite tables are automatically base64 encoded to keep
the downstream processors schema-safe. The pipeline treats common "null"
sentinels such as empty strings and the tokens `null`, `NULL`, and `NaN` as
missing values so data quality metrics do not over-count placeholder entries.

These ingestion paths cover most tabular datasets, but truly arbitrary files
are out of scope. Proprietary binary formats, password-protected archives, and
multi-terabyte inputs will fail gracefully with surfaced errors so you can
decide whether to transform the data upstream. When a file cannot be parsed the
job lands in the `FAILED` state and emits an error artifact detailing the root
cause.

### Connector landscape

Jobs are staged from S3 objects that you upload or copy into the artifacts
bucket. The FastAPI surface provides presigned URLs for direct browser uploads
(`POST /jobs`), and you can also point a job at an existing `s3://bucket/key`
pair by setting the job's `source` metadata. Additional connectors (HTTP
endpoints, JDBC databases, and warehouse-native readers) are planned but not
yet implemented in this repository snapshot.

## Analytics pipeline

Once a job is staged, invoking `POST /jobs/{jobId}/process` streams the real
LangGraph analytics pipeline instead of placeholder outputs. The handler loads
the staged object from S3, routes it through the pipeline phases, and
persistent artifacts and manifests to `artifacts/{jobId}/`. Every phase update
is pushed back into DynamoDB so the dashboard and API clients can display live
progress. When the pipeline finishes, genuine descriptive statistics,
correlations, outlier diagnostics, inference hints, and rendered visualisations
are bundled into `results.json` for downstream consumption.

The pipeline is defensive against messy data: schema inference treats zero-only
columns and sparsely populated fields as categorical, metrics such as dataset
completeness exclude recognised null sentinels, and quantile / standard
deviation calculations use numerically stable streaming algorithms. Extremely
large files may still exceed the Lambda's available memory or timeout budget,
but those cases surface explicit failure events and error artifacts so you can
iterate safely.

## API hardening & observability

The FastAPI service fronts the ingestion workflow and exposes endpoints for
status introspection and artifact browsing:

* `GET /jobs/{jobId}` – returns job status, timestamps, result key metadata, and
  source information.
* `GET /jobs/{jobId}/manifest` – fetches the manifest JSON captured during
  ingestion for inspection.
* `GET /jobs/{jobId}/artifacts` – lists objects stored under the job's artifact
  prefix with optional pagination and hierarchical filtering.
* `GET /jobs/{jobId}/results/files` – enumerates published result files beneath
  `artifacts/{jobId}/results/`.
* `GET /jobs/{jobId}/results` – issues a signed download URL for either the
  default `results.json` or any specific result file path.

Each request is wrapped with structured logging and emits CloudWatch-style
metrics for successful and failed job submissions, giving visibility into
latency, validation failures, and workflow launches.

## Web dashboard

A companion Next.js dashboard lives in [`dashboard/`](dashboard/) for operations teams that prefer a visual control plane.
It supports:

* Drag-and-drop dataset uploads via presigned S3 URLs generated by `POST /jobs`.
* S3-backed job creation when data already exists in your buckets.
* Realtime job monitoring with automatic polling of `GET /jobs/{jobId}`.
* Manifest inspection, artifact browsing, and one-click signed downloads for published results.

### Running locally

The Next.js source lives directly under [`dashboard/`](dashboard/). If you do not
see that directory after cloning the repository, double-check that you are on an
up-to-date branch (`git pull origin main`) and that your checkout completed
successfully. A missing folder indicates your local copy predates the dashboard
being added.

Once the directory is present you can work from inside it:

```bash
cd dashboard
npm install
# Point the UI at your running API (defaults to http://localhost:8000)
NEXT_PUBLIC_API_BASE_URL="https://your-api.example.com" npm run dev
```

Alternatively, stay at the repository root and use the helper scripts that proxy
into the dashboard workspace:

```bash
npm run dashboard:install
# Point the UI at your running API (defaults to http://localhost:8000)
NEXT_PUBLIC_API_BASE_URL="https://your-api.example.com" npm run dashboard:dev
```

The dashboard persists recently viewed job IDs in local storage so you can revisit
completed runs without re-querying DynamoDB.
